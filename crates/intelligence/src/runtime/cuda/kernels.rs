//! PTX kernel sources for CUDA compute backend.
//!
//! All kernels target sm_50 for broad compatibility (Maxwell and newer).
//! PTX is NVIDIA's parallel thread execution virtual ISA.
//!
//! Kernels are bundled into a single PTX module string that is loaded once
//! via `cuModuleLoadData` at backend initialization time.

/// Complete PTX module containing all compute kernels.
///
/// The module is compiled for `sm_50` (Maxwell) to maximize compatibility.
/// All kernels use 32-bit floats (`f32`) for weights and activations.
pub const PTX_MODULE: &str = concat!(
    // -----------------------------------------------------------------------
    // PTX header
    // -----------------------------------------------------------------------
    ".version 6.0\n",
    ".target sm_50\n",
    ".address_size 64\n",
    "\n",
    // -----------------------------------------------------------------------
    // gemm: C = A * B  (tiled 16x16 shared-memory GEMM)
    //
    // Parameters (in order):
    //   %rd_A   : .u64  pointer to A (M x K, row-major)
    //   %rd_B   : .u64  pointer to B (K x N, row-major)
    //   %rd_C   : .u64  pointer to C (M x N, row-major)
    //   %r_M    : .u32  number of rows in A / C
    //   %r_K    : .u32  inner dimension
    //   %r_N    : .u32  number of cols in B / C
    //
    // Grid:  (ceil(N/16), ceil(M/16), 1)
    // Block: (16, 16, 1)
    // -----------------------------------------------------------------------
    ".visible .entry gemm(\n",
    "    .param .u64 param_A,\n",
    "    .param .u64 param_B,\n",
    "    .param .u64 param_C,\n",
    "    .param .u32 param_M,\n",
    "    .param .u32 param_K,\n",
    "    .param .u32 param_N\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<20>;\n",
    "    .reg .u32 %r<30>;\n",
    "    .reg .f32 %f<10>;\n",
    "    .reg .pred %p<5>;\n",
    "    .shared .align 4 .f32 tile_A[256];\n", // 16*16
    "    .shared .align 4 .f32 tile_B[256];\n", // 16*16
    "\n",
    "    // Load parameters\n",
    "    ld.param.u64 %rd0, [param_A];\n",
    "    ld.param.u64 %rd1, [param_B];\n",
    "    ld.param.u64 %rd2, [param_C];\n",
    "    ld.param.u32 %r0, [param_M];\n",
    "    ld.param.u32 %r1, [param_K];\n",
    "    ld.param.u32 %r2, [param_N];\n",
    "\n",
    "    // Thread indices\n",
    "    mov.u32 %r3, %tid.x;         // tx (column within tile)\n",
    "    mov.u32 %r4, %tid.y;         // ty (row within tile)\n",
    "    mov.u32 %r5, %ctaid.x;       // bx (block col)\n",
    "    mov.u32 %r6, %ctaid.y;       // by (block row)\n",
    "\n",
    "    // Global row/col for this thread\n",
    "    shl.b32 %r7, %r6, 4;         // by * 16\n",
    "    add.u32 %r7, %r7, %r4;       // row = by*16 + ty\n",
    "    shl.b32 %r8, %r5, 4;         // bx * 16\n",
    "    add.u32 %r8, %r8, %r3;       // col = bx*16 + tx\n",
    "\n",
    "    // Accumulator\n",
    "    mov.f32 %f0, 0f00000000;\n", // acc = 0.0
    "\n",
    "    // Shared memory index for this thread: ty*16+tx\n",
    "    shl.b32 %r9, %r4, 4;\n",
    "    add.u32 %r9, %r9, %r3;       // shmem_idx = ty*16+tx\n",
    "\n",
    "    // Tile loop: t = 0, 16, 32, ...\n",
    "    mov.u32 %r10, 0;             // t = 0\n",
    "GEMM_TILE_LOOP:\n",
    "    // --- Load tile_A[ty][tx] = A[row][t+tx] ---\n",
    "    add.u32 %r11, %r10, %r3;     // t + tx\n",
    "    setp.lt.u32 %p0, %r7, %r0;   // row < M?\n",
    "    setp.lt.u32 %p1, %r11, %r1;  // t+tx < K?\n",
    "    and.pred %p2, %p0, %p1;\n",
    "    @!%p2 bra GEMM_ZERO_A;\n",
    "    // A offset: row*K + (t+tx)\n",
    "    mul.lo.u32 %r12, %r7, %r1;\n",
    "    add.u32 %r12, %r12, %r11;\n",
    "    mul.wide.u32 %rd3, %r12, 4;\n",
    "    add.u64 %rd3, %rd0, %rd3;\n",
    "    ld.global.f32 %f1, [%rd3];\n",
    "    bra GEMM_STORE_A;\n",
    "GEMM_ZERO_A:\n",
    "    mov.f32 %f1, 0f00000000;\n",
    "GEMM_STORE_A:\n",
    "    mov.u32 %r13, tile_A;\n",
    "    shl.b32 %r14, %r9, 2;        // shmem_idx * 4\n",
    "    add.u32 %r13, %r13, %r14;\n",
    "    st.shared.f32 [%r13], %f1;\n",
    "\n",
    "    // --- Load tile_B[ty][tx] = B[t+ty][col] ---\n",
    "    add.u32 %r15, %r10, %r4;     // t + ty\n",
    "    setp.lt.u32 %p0, %r15, %r1;  // t+ty < K?\n",
    "    setp.lt.u32 %p1, %r8, %r2;   // col < N?\n",
    "    and.pred %p2, %p0, %p1;\n",
    "    @!%p2 bra GEMM_ZERO_B;\n",
    "    // B offset: (t+ty)*N + col\n",
    "    mul.lo.u32 %r16, %r15, %r2;\n",
    "    add.u32 %r16, %r16, %r8;\n",
    "    mul.wide.u32 %rd4, %r16, 4;\n",
    "    add.u64 %rd4, %rd1, %rd4;\n",
    "    ld.global.f32 %f2, [%rd4];\n",
    "    bra GEMM_STORE_B;\n",
    "GEMM_ZERO_B:\n",
    "    mov.f32 %f2, 0f00000000;\n",
    "GEMM_STORE_B:\n",
    "    mov.u32 %r17, tile_B;\n",
    "    shl.b32 %r18, %r9, 2;\n",
    "    add.u32 %r17, %r17, %r18;\n",
    "    st.shared.f32 [%r17], %f2;\n",
    "\n",
    "    bar.sync 0;\n",
    "\n",
    "    // --- Accumulate: acc += tile_A[ty][k] * tile_B[k][tx] ---\n",
    "    mov.u32 %r19, 0;             // k = 0\n",
    "GEMM_K_LOOP:\n",
    "    // tile_A[ty*16 + k]\n",
    "    shl.b32 %r20, %r4, 4;\n",
    "    add.u32 %r20, %r20, %r19;\n",
    "    shl.b32 %r20, %r20, 2;\n",
    "    mov.u32 %r21, tile_A;\n",
    "    add.u32 %r21, %r21, %r20;\n",
    "    ld.shared.f32 %f3, [%r21];\n",
    "    // tile_B[k*16 + tx]\n",
    "    shl.b32 %r22, %r19, 4;\n",
    "    add.u32 %r22, %r22, %r3;\n",
    "    shl.b32 %r22, %r22, 2;\n",
    "    mov.u32 %r23, tile_B;\n",
    "    add.u32 %r23, %r23, %r22;\n",
    "    ld.shared.f32 %f4, [%r23];\n",
    "    fma.rn.f32 %f0, %f3, %f4, %f0;\n",
    "    add.u32 %r19, %r19, 1;\n",
    "    setp.lt.u32 %p0, %r19, 16;\n",
    "    @%p0 bra GEMM_K_LOOP;\n",
    "\n",
    "    bar.sync 0;\n",
    "\n",
    "    add.u32 %r10, %r10, 16;\n",
    "    setp.lt.u32 %p0, %r10, %r1;  // t < K\n",
    "    @%p0 bra GEMM_TILE_LOOP;\n",
    "\n",
    "    // --- Write result ---\n",
    "    setp.lt.u32 %p0, %r7, %r0;   // row < M\n",
    "    setp.lt.u32 %p1, %r8, %r2;   // col < N\n",
    "    and.pred %p2, %p0, %p1;\n",
    "    @!%p2 bra GEMM_DONE;\n",
    "    mul.lo.u32 %r24, %r7, %r2;\n",
    "    add.u32 %r24, %r24, %r8;\n",
    "    mul.wide.u32 %rd5, %r24, 4;\n",
    "    add.u64 %rd5, %rd2, %rd5;\n",
    "    st.global.f32 [%rd5], %f0;\n",
    "GEMM_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // gemm_transpose: C = A * B^T  (tiled 16x16, B is (N,K) read transposed)
    //
    // Parameters: same as gemm — A (M,K), B (N,K), C (M,N), M, K, N
    // Grid:  (ceil(N/16), ceil(M/16), 1)
    // Block: (16, 16, 1)
    // -----------------------------------------------------------------------
    ".visible .entry gemm_transpose(\n",
    "    .param .u64 param_A,\n",
    "    .param .u64 param_B,\n",
    "    .param .u64 param_C,\n",
    "    .param .u32 param_M,\n",
    "    .param .u32 param_K,\n",
    "    .param .u32 param_N\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<20>;\n",
    "    .reg .u32 %r<30>;\n",
    "    .reg .f32 %f<10>;\n",
    "    .reg .pred %p<5>;\n",
    "    .shared .align 4 .f32 tile_A[256];\n",
    "    .shared .align 4 .f32 tile_B[256];\n",
    "\n",
    "    ld.param.u64 %rd0, [param_A];\n",
    "    ld.param.u64 %rd1, [param_B];\n",
    "    ld.param.u64 %rd2, [param_C];\n",
    "    ld.param.u32 %r0, [param_M];\n",
    "    ld.param.u32 %r1, [param_K];\n",
    "    ld.param.u32 %r2, [param_N];\n",
    "\n",
    "    mov.u32 %r3, %tid.x;         // tx\n",
    "    mov.u32 %r4, %tid.y;         // ty\n",
    "    mov.u32 %r5, %ctaid.x;       // bx\n",
    "    mov.u32 %r6, %ctaid.y;       // by\n",
    "\n",
    "    shl.b32 %r7, %r6, 4;\n",
    "    add.u32 %r7, %r7, %r4;       // row = by*16 + ty\n",
    "    shl.b32 %r8, %r5, 4;\n",
    "    add.u32 %r8, %r8, %r3;       // col = bx*16 + tx\n",
    "\n",
    "    mov.f32 %f0, 0f00000000;\n",
    "    shl.b32 %r9, %r4, 4;\n",
    "    add.u32 %r9, %r9, %r3;\n",
    "\n",
    "    mov.u32 %r10, 0;\n",
    "GEMMT_TILE_LOOP:\n",
    "    // tile_A[ty][tx] = A[row][t+tx]\n",
    "    add.u32 %r11, %r10, %r3;\n",
    "    setp.lt.u32 %p0, %r7, %r0;\n",
    "    setp.lt.u32 %p1, %r11, %r1;\n",
    "    and.pred %p2, %p0, %p1;\n",
    "    @!%p2 bra GEMMT_ZERO_A;\n",
    "    mul.lo.u32 %r12, %r7, %r1;\n",
    "    add.u32 %r12, %r12, %r11;\n",
    "    mul.wide.u32 %rd3, %r12, 4;\n",
    "    add.u64 %rd3, %rd0, %rd3;\n",
    "    ld.global.f32 %f1, [%rd3];\n",
    "    bra GEMMT_STORE_A;\n",
    "GEMMT_ZERO_A:\n",
    "    mov.f32 %f1, 0f00000000;\n",
    "GEMMT_STORE_A:\n",
    "    mov.u32 %r13, tile_A;\n",
    "    shl.b32 %r14, %r9, 2;\n",
    "    add.u32 %r13, %r13, %r14;\n",
    "    st.shared.f32 [%r13], %f1;\n",
    "\n",
    "    // tile_B[ty][tx] = B[col][t+ty]  (B is transposed: stored as (N,K))\n",
    "    add.u32 %r15, %r10, %r4;     // t + ty\n",
    "    setp.lt.u32 %p0, %r8, %r2;   // col < N?\n",
    "    setp.lt.u32 %p1, %r15, %r1;  // t+ty < K?\n",
    "    and.pred %p2, %p0, %p1;\n",
    "    @!%p2 bra GEMMT_ZERO_B;\n",
    "    // B offset: col*K + (t+ty)  (B is (N,K) row-major)\n",
    "    mul.lo.u32 %r16, %r8, %r1;\n",
    "    add.u32 %r16, %r16, %r15;\n",
    "    mul.wide.u32 %rd4, %r16, 4;\n",
    "    add.u64 %rd4, %rd1, %rd4;\n",
    "    ld.global.f32 %f2, [%rd4];\n",
    "    bra GEMMT_STORE_B;\n",
    "GEMMT_ZERO_B:\n",
    "    mov.f32 %f2, 0f00000000;\n",
    "GEMMT_STORE_B:\n",
    "    mov.u32 %r17, tile_B;\n",
    "    shl.b32 %r18, %r9, 2;\n",
    "    add.u32 %r17, %r17, %r18;\n",
    "    st.shared.f32 [%r17], %f2;\n",
    "\n",
    "    bar.sync 0;\n",
    "\n",
    "    // Accumulate: acc += tile_A[ty][k] * tile_B[k][tx]\n",
    "    // tile_B[k][tx] corresponds to B[col][t+k] - transposed access\n",
    "    mov.u32 %r19, 0;\n",
    "GEMMT_K_LOOP:\n",
    "    shl.b32 %r20, %r4, 4;\n",
    "    add.u32 %r20, %r20, %r19;\n",
    "    shl.b32 %r20, %r20, 2;\n",
    "    mov.u32 %r21, tile_A;\n",
    "    add.u32 %r21, %r21, %r20;\n",
    "    ld.shared.f32 %f3, [%r21];\n",
    "    // tile_B: we stored B[col][t+ty] at [ty][tx] but we need B[col][t+k]\n",
    "    // Remap: tile_B was loaded with ty-indexed rows, so tile_B[k][tx]\n",
    "    shl.b32 %r22, %r19, 4;\n",
    "    add.u32 %r22, %r22, %r3;\n",
    "    shl.b32 %r22, %r22, 2;\n",
    "    mov.u32 %r23, tile_B;\n",
    "    add.u32 %r23, %r23, %r22;\n",
    "    ld.shared.f32 %f4, [%r23];\n",
    "    fma.rn.f32 %f0, %f3, %f4, %f0;\n",
    "    add.u32 %r19, %r19, 1;\n",
    "    setp.lt.u32 %p0, %r19, 16;\n",
    "    @%p0 bra GEMMT_K_LOOP;\n",
    "\n",
    "    bar.sync 0;\n",
    "\n",
    "    add.u32 %r10, %r10, 16;\n",
    "    setp.lt.u32 %p0, %r10, %r1;\n",
    "    @%p0 bra GEMMT_TILE_LOOP;\n",
    "\n",
    "    setp.lt.u32 %p0, %r7, %r0;\n",
    "    setp.lt.u32 %p1, %r8, %r2;\n",
    "    and.pred %p2, %p0, %p1;\n",
    "    @!%p2 bra GEMMT_DONE;\n",
    "    mul.lo.u32 %r24, %r7, %r2;\n",
    "    add.u32 %r24, %r24, %r8;\n",
    "    mul.wide.u32 %rd5, %r24, 4;\n",
    "    add.u64 %rd5, %rd2, %rd5;\n",
    "    st.global.f32 [%rd5], %f0;\n",
    "GEMMT_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // gelu: element-wise GELU approximation
    //   y = x * 0.5 * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))
    //
    // Parameters: input ptr (.u64), output ptr (.u64), n (.u32)
    // Grid: (ceil(n/256), 1, 1), Block: (256, 1, 1)
    //
    // tanh(a) via: tanh(a) = 1 - 2/(exp(2a)+1)
    //   exp(2a) = exp2(2a * log2(e))
    // -----------------------------------------------------------------------
    ".visible .entry gelu(\n",
    "    .param .u64 param_in,\n",
    "    .param .u64 param_out,\n",
    "    .param .u32 param_n\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<6>;\n",
    "    .reg .u32 %r<6>;\n",
    "    .reg .f32 %f<20>;\n",
    "    .reg .pred %p0;\n",
    "\n",
    "    ld.param.u64 %rd0, [param_in];\n",
    "    ld.param.u64 %rd1, [param_out];\n",
    "    ld.param.u32 %r0, [param_n];\n",
    "\n",
    "    mov.u32 %r1, %ctaid.x;\n",
    "    mov.u32 %r2, %ntid.x;\n",
    "    mul.lo.u32 %r1, %r1, %r2;\n",
    "    mov.u32 %r3, %tid.x;\n",
    "    add.u32 %r1, %r1, %r3;       // idx = blockIdx.x * blockDim.x + threadIdx.x\n",
    "    setp.ge.u32 %p0, %r1, %r0;\n",
    "    @%p0 bra GELU_DONE;\n",
    "\n",
    "    // Load x\n",
    "    mul.wide.u32 %rd2, %r1, 4;\n",
    "    add.u64 %rd3, %rd0, %rd2;\n",
    "    ld.global.f32 %f0, [%rd3];    // x\n",
    "\n",
    "    // Constants\n",
    "    mov.f32 %f1, 0f3F4C422A;      // sqrt(2/pi) = 0.7978845608\n",
    "    mov.f32 %f2, 0f3D372713;      // 0.044715\n",
    "    mov.f32 %f3, 0f3F000000;      // 0.5\n",
    "    mov.f32 %f10, 0f3FB8AA3B;     // log2(e) = 1.4426950408\n",
    "    mov.f32 %f11, 0f40000000;     // 2.0\n",
    "    mov.f32 %f12, 0f3F800000;     // 1.0\n",
    "\n",
    "    // x^3\n",
    "    mul.rn.f32 %f4, %f0, %f0;     // x^2\n",
    "    mul.rn.f32 %f4, %f4, %f0;     // x^3\n",
    "    // 0.044715 * x^3\n",
    "    mul.rn.f32 %f4, %f2, %f4;\n",
    "    // x + 0.044715 * x^3\n",
    "    add.rn.f32 %f4, %f0, %f4;\n",
    "    // sqrt(2/pi) * (x + 0.044715 * x^3)\n",
    "    mul.rn.f32 %f4, %f1, %f4;     // a = inner arg to tanh\n",
    "\n",
    "    // tanh(a) = 1 - 2/(exp(2a)+1)\n",
    "    // exp(2a) = exp2(2a * log2(e))\n",
    "    mul.rn.f32 %f5, %f4, %f11;    // 2a\n",
    "    mul.rn.f32 %f5, %f5, %f10;    // 2a * log2(e)\n",
    "    ex2.approx.f32 %f5, %f5;      // exp2(2a * log2(e)) = exp(2a)\n",
    "    add.rn.f32 %f6, %f5, %f12;    // exp(2a) + 1\n",
    "    div.approx.f32 %f6, %f11, %f6; // 2 / (exp(2a)+1)\n",
    "    sub.rn.f32 %f7, %f12, %f6;    // tanh(a) = 1 - 2/(exp(2a)+1)\n",
    "\n",
    "    // y = 0.5 * x * (1 + tanh)\n",
    "    add.rn.f32 %f8, %f12, %f7;    // 1 + tanh(a)\n",
    "    mul.rn.f32 %f8, %f3, %f8;     // 0.5 * (1 + tanh(a))\n",
    "    mul.rn.f32 %f9, %f0, %f8;     // x * 0.5 * (1 + tanh(a))\n",
    "\n",
    "    // Store\n",
    "    add.u64 %rd4, %rd1, %rd2;\n",
    "    st.global.f32 [%rd4], %f9;\n",
    "GELU_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // add_tensor: c[i] = a[i] + b[i]
    //
    // Parameters: a ptr (.u64), b ptr (.u64), c ptr (.u64), n (.u32)
    // Grid: (ceil(n/256), 1, 1), Block: (256, 1, 1)
    // -----------------------------------------------------------------------
    ".visible .entry add_tensor(\n",
    "    .param .u64 param_a,\n",
    "    .param .u64 param_b,\n",
    "    .param .u64 param_c,\n",
    "    .param .u32 param_n\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<8>;\n",
    "    .reg .u32 %r<5>;\n",
    "    .reg .f32 %f<4>;\n",
    "    .reg .pred %p0;\n",
    "\n",
    "    ld.param.u64 %rd0, [param_a];\n",
    "    ld.param.u64 %rd1, [param_b];\n",
    "    ld.param.u64 %rd2, [param_c];\n",
    "    ld.param.u32 %r0, [param_n];\n",
    "\n",
    "    mov.u32 %r1, %ctaid.x;\n",
    "    mov.u32 %r2, %ntid.x;\n",
    "    mul.lo.u32 %r1, %r1, %r2;\n",
    "    mov.u32 %r3, %tid.x;\n",
    "    add.u32 %r1, %r1, %r3;\n",
    "    setp.ge.u32 %p0, %r1, %r0;\n",
    "    @%p0 bra ADD_DONE;\n",
    "\n",
    "    mul.wide.u32 %rd3, %r1, 4;\n",
    "    add.u64 %rd4, %rd0, %rd3;\n",
    "    add.u64 %rd5, %rd1, %rd3;\n",
    "    add.u64 %rd6, %rd2, %rd3;\n",
    "    ld.global.f32 %f0, [%rd4];\n",
    "    ld.global.f32 %f1, [%rd5];\n",
    "    add.rn.f32 %f2, %f0, %f1;\n",
    "    st.global.f32 [%rd6], %f2;\n",
    "ADD_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // add_bias: t[r*cols + c] += bias[c]
    //
    // Parameters: t ptr (.u64), bias ptr (.u64), rows (.u32), cols (.u32)
    // Grid: (rows, ceil(cols/256), 1), Block: (256, 1, 1)
    // -----------------------------------------------------------------------
    ".visible .entry add_bias(\n",
    "    .param .u64 param_t,\n",
    "    .param .u64 param_bias,\n",
    "    .param .u32 param_rows,\n",
    "    .param .u32 param_cols\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<8>;\n",
    "    .reg .u32 %r<8>;\n",
    "    .reg .f32 %f<4>;\n",
    "    .reg .pred %p0;\n",
    "\n",
    "    ld.param.u64 %rd0, [param_t];\n",
    "    ld.param.u64 %rd1, [param_bias];\n",
    "    ld.param.u32 %r0, [param_rows];\n",
    "    ld.param.u32 %r1, [param_cols];\n",
    "\n",
    "    // r = blockIdx.x, c = blockIdx.y * blockDim.x + threadIdx.x\n",
    "    mov.u32 %r5, %ctaid.x;       // r\n",
    "    mov.u32 %r2, %ctaid.y;\n",
    "    mov.u32 %r3, %ntid.x;\n",
    "    mul.lo.u32 %r2, %r2, %r3;\n",
    "    mov.u32 %r4, %tid.x;\n",
    "    add.u32 %r2, %r2, %r4;       // c\n",
    "\n",
    "    setp.ge.u32 %p0, %r2, %r1;\n",
    "    @%p0 bra BIAS_DONE;\n",
    "\n",
    "    // offset = r * cols + c\n",
    "    mul.lo.u32 %r6, %r5, %r1;\n",
    "    add.u32 %r6, %r6, %r2;\n",
    "    mul.wide.u32 %rd2, %r6, 4;\n",
    "    add.u64 %rd3, %rd0, %rd2;\n",
    "    ld.global.f32 %f0, [%rd3];\n",
    "\n",
    "    // bias[c]\n",
    "    mul.wide.u32 %rd4, %r2, 4;\n",
    "    add.u64 %rd5, %rd1, %rd4;\n",
    "    ld.global.f32 %f1, [%rd5];\n",
    "\n",
    "    add.rn.f32 %f2, %f0, %f1;\n",
    "    st.global.f32 [%rd3], %f2;\n",
    "BIAS_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // scale: t[i] *= factor
    //
    // Parameters: t ptr (.u64), factor (.f32), n (.u32)
    // Grid: (ceil(n/256), 1, 1), Block: (256, 1, 1)
    // -----------------------------------------------------------------------
    ".visible .entry scale(\n",
    "    .param .u64 param_t,\n",
    "    .param .f32 param_factor,\n",
    "    .param .u32 param_n\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<4>;\n",
    "    .reg .u32 %r<5>;\n",
    "    .reg .f32 %f<3>;\n",
    "    .reg .pred %p0;\n",
    "\n",
    "    ld.param.u64 %rd0, [param_t];\n",
    "    ld.param.f32 %f0, [param_factor];\n",
    "    ld.param.u32 %r0, [param_n];\n",
    "\n",
    "    mov.u32 %r1, %ctaid.x;\n",
    "    mov.u32 %r2, %ntid.x;\n",
    "    mul.lo.u32 %r1, %r1, %r2;\n",
    "    mov.u32 %r3, %tid.x;\n",
    "    add.u32 %r1, %r1, %r3;\n",
    "    setp.ge.u32 %p0, %r1, %r0;\n",
    "    @%p0 bra SCALE_DONE;\n",
    "\n",
    "    mul.wide.u32 %rd1, %r1, 4;\n",
    "    add.u64 %rd2, %rd0, %rd1;\n",
    "    ld.global.f32 %f1, [%rd2];\n",
    "    mul.rn.f32 %f2, %f1, %f0;\n",
    "    st.global.f32 [%rd2], %f2;\n",
    "SCALE_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // layer_norm: per-row normalization with weight and bias
    //
    // Parameters:
    //   input ptr (.u64), output ptr (.u64),
    //   weight ptr (.u64), bias ptr (.u64),
    //   rows (.u32), cols (.u32), eps (.f32)
    //
    // Grid:  (rows, 1, 1)   — one block per row
    // Block: (256, 1, 1)
    //
    // Algorithm per row:
    //   1. Parallel reduction for sum (mean)
    //   2. Parallel reduction for variance
    //   3. Normalize: out[c] = (in[c] - mean) * rsqrt(var + eps) * w[c] + b[c]
    // -----------------------------------------------------------------------
    ".visible .entry layer_norm(\n",
    "    .param .u64 param_in,\n",
    "    .param .u64 param_out,\n",
    "    .param .u64 param_w,\n",
    "    .param .u64 param_b,\n",
    "    .param .u32 param_rows,\n",
    "    .param .u32 param_cols,\n",
    "    .param .f32 param_eps\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<20>;\n",
    "    .reg .u32 %r<20>;\n",
    "    .reg .f32 %f<20>;\n",
    "    .reg .pred %p<4>;\n",
    "    .shared .align 4 .f32 sdata[256];\n",
    "\n",
    "    ld.param.u64 %rd0, [param_in];\n",
    "    ld.param.u64 %rd1, [param_out];\n",
    "    ld.param.u64 %rd2, [param_w];\n",
    "    ld.param.u64 %rd3, [param_b];\n",
    "    ld.param.u32 %r0, [param_rows];\n",
    "    ld.param.u32 %r1, [param_cols];\n",
    "    ld.param.f32 %f0, [param_eps];\n",
    "\n",
    "    mov.u32 %r2, %ctaid.x;       // row index\n",
    "    mov.u32 %r3, %tid.x;         // thread index\n",
    "    mov.u32 %r4, %ntid.x;        // blockDim.x = 256\n",
    "\n",
    "    // Base pointer for this row's input\n",
    "    mul.lo.u32 %r5, %r2, %r1;    // row * cols\n",
    "    mul.wide.u32 %rd4, %r5, 4;\n",
    "    add.u64 %rd5, %rd0, %rd4;    // &input[row * cols]\n",
    "\n",
    "    // --- Phase 1: compute mean via parallel reduction ---\n",
    "    // Each thread sums elements at stride blockDim.x\n",
    "    mov.f32 %f1, 0f00000000;     // partial_sum = 0\n",
    "    mov.u32 %r6, %r3;            // i = tid\n",
    "LN_SUM_LOOP:\n",
    "    setp.ge.u32 %p0, %r6, %r1;\n",
    "    @%p0 bra LN_SUM_DONE;\n",
    "    mul.wide.u32 %rd6, %r6, 4;\n",
    "    add.u64 %rd7, %rd5, %rd6;\n",
    "    ld.global.f32 %f2, [%rd7];\n",
    "    add.rn.f32 %f1, %f1, %f2;\n",
    "    add.u32 %r6, %r6, %r4;\n",
    "    bra LN_SUM_LOOP;\n",
    "LN_SUM_DONE:\n",
    "\n",
    "    // Store partial sum to shared memory\n",
    "    mov.u32 %r7, sdata;\n",
    "    shl.b32 %r8, %r3, 2;\n",
    "    add.u32 %r7, %r7, %r8;\n",
    "    st.shared.f32 [%r7], %f1;\n",
    "    bar.sync 0;\n",
    "\n",
    "    // Tree reduction\n",
    "    mov.u32 %r9, 128;            // stride\n",
    "LN_RED1:\n",
    "    setp.ge.u32 %p0, %r3, %r9;\n",
    "    @%p0 bra LN_RED1_SKIP;\n",
    "    add.u32 %r10, %r3, %r9;\n",
    "    mov.u32 %r11, sdata;\n",
    "    shl.b32 %r12, %r10, 2;\n",
    "    add.u32 %r11, %r11, %r12;\n",
    "    ld.shared.f32 %f3, [%r11];\n",
    "    mov.u32 %r11, sdata;\n",
    "    shl.b32 %r12, %r3, 2;\n",
    "    add.u32 %r11, %r11, %r12;\n",
    "    ld.shared.f32 %f4, [%r11];\n",
    "    add.rn.f32 %f4, %f4, %f3;\n",
    "    st.shared.f32 [%r11], %f4;\n",
    "LN_RED1_SKIP:\n",
    "    bar.sync 0;\n",
    "    shr.u32 %r9, %r9, 1;\n",
    "    setp.ge.u32 %p0, %r9, 1;\n",
    "    @%p0 bra LN_RED1;\n",
    "\n",
    "    // mean = sdata[0] / cols\n",
    "    mov.u32 %r11, sdata;\n",
    "    ld.shared.f32 %f5, [%r11];   // total sum\n",
    "    cvt.rn.f32.u32 %f6, %r1;     // (float)cols\n",
    "    div.approx.f32 %f5, %f5, %f6; // mean\n",
    "    bar.sync 0;\n",
    "\n",
    "    // --- Phase 2: compute variance via parallel reduction ---\n",
    "    mov.f32 %f7, 0f00000000;     // partial_var = 0\n",
    "    mov.u32 %r6, %r3;\n",
    "LN_VAR_LOOP:\n",
    "    setp.ge.u32 %p0, %r6, %r1;\n",
    "    @%p0 bra LN_VAR_DONE;\n",
    "    mul.wide.u32 %rd6, %r6, 4;\n",
    "    add.u64 %rd7, %rd5, %rd6;\n",
    "    ld.global.f32 %f8, [%rd7];\n",
    "    sub.rn.f32 %f8, %f8, %f5;    // x - mean\n",
    "    fma.rn.f32 %f7, %f8, %f8, %f7; // += (x-mean)^2\n",
    "    add.u32 %r6, %r6, %r4;\n",
    "    bra LN_VAR_LOOP;\n",
    "LN_VAR_DONE:\n",
    "\n",
    "    mov.u32 %r7, sdata;\n",
    "    shl.b32 %r8, %r3, 2;\n",
    "    add.u32 %r7, %r7, %r8;\n",
    "    st.shared.f32 [%r7], %f7;\n",
    "    bar.sync 0;\n",
    "\n",
    "    mov.u32 %r9, 128;\n",
    "LN_RED2:\n",
    "    setp.ge.u32 %p0, %r3, %r9;\n",
    "    @%p0 bra LN_RED2_SKIP;\n",
    "    add.u32 %r10, %r3, %r9;\n",
    "    mov.u32 %r11, sdata;\n",
    "    shl.b32 %r12, %r10, 2;\n",
    "    add.u32 %r11, %r11, %r12;\n",
    "    ld.shared.f32 %f3, [%r11];\n",
    "    mov.u32 %r11, sdata;\n",
    "    shl.b32 %r12, %r3, 2;\n",
    "    add.u32 %r11, %r11, %r12;\n",
    "    ld.shared.f32 %f4, [%r11];\n",
    "    add.rn.f32 %f4, %f4, %f3;\n",
    "    st.shared.f32 [%r11], %f4;\n",
    "LN_RED2_SKIP:\n",
    "    bar.sync 0;\n",
    "    shr.u32 %r9, %r9, 1;\n",
    "    setp.ge.u32 %p0, %r9, 1;\n",
    "    @%p0 bra LN_RED2;\n",
    "\n",
    "    // var = sdata[0] / cols\n",
    "    mov.u32 %r11, sdata;\n",
    "    ld.shared.f32 %f9, [%r11];\n",
    "    div.approx.f32 %f9, %f9, %f6; // variance\n",
    "    // inv_std = rsqrt(var + eps)\n",
    "    add.rn.f32 %f9, %f9, %f0;     // var + eps\n",
    "    rsqrt.approx.f32 %f10, %f9;   // inv_std\n",
    "    bar.sync 0;\n",
    "\n",
    "    // --- Phase 3: normalize and write output ---\n",
    "    add.u64 %rd8, %rd1, %rd4;    // &output[row * cols]\n",
    "    mov.u32 %r6, %r3;\n",
    "LN_NORM_LOOP:\n",
    "    setp.ge.u32 %p0, %r6, %r1;\n",
    "    @%p0 bra LN_NORM_DONE;\n",
    "    // Load input[c]\n",
    "    mul.wide.u32 %rd6, %r6, 4;\n",
    "    add.u64 %rd7, %rd5, %rd6;\n",
    "    ld.global.f32 %f11, [%rd7];\n",
    "    // (x - mean) * inv_std\n",
    "    sub.rn.f32 %f11, %f11, %f5;\n",
    "    mul.rn.f32 %f11, %f11, %f10;\n",
    "    // * weight[c]\n",
    "    add.u64 %rd9, %rd2, %rd6;\n",
    "    ld.global.f32 %f12, [%rd9];\n",
    "    mul.rn.f32 %f11, %f11, %f12;\n",
    "    // + bias[c]\n",
    "    add.u64 %rd10, %rd3, %rd6;\n",
    "    ld.global.f32 %f13, [%rd10];\n",
    "    add.rn.f32 %f11, %f11, %f13;\n",
    "    // Store\n",
    "    add.u64 %rd11, %rd8, %rd6;\n",
    "    st.global.f32 [%rd11], %f11;\n",
    "    add.u32 %r6, %r6, %r4;\n",
    "    bra LN_NORM_LOOP;\n",
    "LN_NORM_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // softmax_rows: per-row softmax with max subtraction
    //
    // Parameters: data ptr (.u64), rows (.u32), cols (.u32)
    // Grid:  (rows, 1, 1)
    // Block: (256, 1, 1)
    // -----------------------------------------------------------------------
    ".visible .entry softmax_rows(\n",
    "    .param .u64 param_data,\n",
    "    .param .u32 param_rows,\n",
    "    .param .u32 param_cols\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<10>;\n",
    "    .reg .u32 %r<16>;\n",
    "    .reg .f32 %f<16>;\n",
    "    .reg .pred %p<4>;\n",
    "    .shared .align 4 .f32 sdata[256];\n",
    "\n",
    "    ld.param.u64 %rd0, [param_data];\n",
    "    ld.param.u32 %r0, [param_rows];\n",
    "    ld.param.u32 %r1, [param_cols];\n",
    "\n",
    "    mov.u32 %r2, %ctaid.x;       // row\n",
    "    mov.u32 %r3, %tid.x;\n",
    "    mov.u32 %r4, %ntid.x;        // 256\n",
    "\n",
    "    // Row base pointer\n",
    "    mul.lo.u32 %r5, %r2, %r1;\n",
    "    mul.wide.u32 %rd1, %r5, 4;\n",
    "    add.u64 %rd2, %rd0, %rd1;    // &data[row * cols]\n",
    "\n",
    "    // --- Phase 1: find max ---\n",
    "    mov.f32 %f0, 0fFF7FFFFF;     // -FLT_MAX as starting max\n",
    "    mov.u32 %r6, %r3;\n",
    "SM_MAX_LOOP:\n",
    "    setp.ge.u32 %p0, %r6, %r1;\n",
    "    @%p0 bra SM_MAX_DONE;\n",
    "    mul.wide.u32 %rd3, %r6, 4;\n",
    "    add.u64 %rd4, %rd2, %rd3;\n",
    "    ld.global.f32 %f1, [%rd4];\n",
    "    max.f32 %f0, %f0, %f1;\n",
    "    add.u32 %r6, %r6, %r4;\n",
    "    bra SM_MAX_LOOP;\n",
    "SM_MAX_DONE:\n",
    "\n",
    "    mov.u32 %r7, sdata;\n",
    "    shl.b32 %r8, %r3, 2;\n",
    "    add.u32 %r7, %r7, %r8;\n",
    "    st.shared.f32 [%r7], %f0;\n",
    "    bar.sync 0;\n",
    "\n",
    "    // Tree reduction for max\n",
    "    mov.u32 %r9, 128;\n",
    "SM_RED_MAX:\n",
    "    setp.ge.u32 %p0, %r3, %r9;\n",
    "    @%p0 bra SM_RED_MAX_SKIP;\n",
    "    add.u32 %r10, %r3, %r9;\n",
    "    mov.u32 %r11, sdata;\n",
    "    shl.b32 %r12, %r10, 2;\n",
    "    add.u32 %r11, %r11, %r12;\n",
    "    ld.shared.f32 %f2, [%r11];\n",
    "    mov.u32 %r11, sdata;\n",
    "    shl.b32 %r12, %r3, 2;\n",
    "    add.u32 %r11, %r11, %r12;\n",
    "    ld.shared.f32 %f3, [%r11];\n",
    "    max.f32 %f3, %f3, %f2;\n",
    "    st.shared.f32 [%r11], %f3;\n",
    "SM_RED_MAX_SKIP:\n",
    "    bar.sync 0;\n",
    "    shr.u32 %r9, %r9, 1;\n",
    "    setp.ge.u32 %p0, %r9, 1;\n",
    "    @%p0 bra SM_RED_MAX;\n",
    "\n",
    "    // Broadcast max from sdata[0]\n",
    "    mov.u32 %r11, sdata;\n",
    "    ld.shared.f32 %f4, [%r11];   // row_max\n",
    "    bar.sync 0;\n",
    "\n",
    "    // --- Phase 2: compute exp(x - max) in-place and sum ---\n",
    "    mov.f32 %f12, 0f3FB8AA3B;    // log2(e)\n",
    "    mov.f32 %f5, 0f00000000;     // partial_sum = 0\n",
    "    mov.u32 %r6, %r3;\n",
    "SM_EXP_LOOP:\n",
    "    setp.ge.u32 %p0, %r6, %r1;\n",
    "    @%p0 bra SM_EXP_DONE;\n",
    "    mul.wide.u32 %rd3, %r6, 4;\n",
    "    add.u64 %rd4, %rd2, %rd3;\n",
    "    ld.global.f32 %f6, [%rd4];\n",
    "    sub.rn.f32 %f6, %f6, %f4;    // x - max\n",
    "    // exp(x-max) = exp2((x-max)*log2(e))\n",
    "    mul.rn.f32 %f7, %f6, %f12;\n",
    "    ex2.approx.f32 %f7, %f7;\n",
    "    st.global.f32 [%rd4], %f7;   // store exp(x-max)\n",
    "    add.rn.f32 %f5, %f5, %f7;\n",
    "    add.u32 %r6, %r6, %r4;\n",
    "    bra SM_EXP_LOOP;\n",
    "SM_EXP_DONE:\n",
    "\n",
    "    mov.u32 %r7, sdata;\n",
    "    shl.b32 %r8, %r3, 2;\n",
    "    add.u32 %r7, %r7, %r8;\n",
    "    st.shared.f32 [%r7], %f5;\n",
    "    bar.sync 0;\n",
    "\n",
    "    // Tree reduction for sum\n",
    "    mov.u32 %r9, 128;\n",
    "SM_RED_SUM:\n",
    "    setp.ge.u32 %p0, %r3, %r9;\n",
    "    @%p0 bra SM_RED_SUM_SKIP;\n",
    "    add.u32 %r10, %r3, %r9;\n",
    "    mov.u32 %r11, sdata;\n",
    "    shl.b32 %r12, %r10, 2;\n",
    "    add.u32 %r11, %r11, %r12;\n",
    "    ld.shared.f32 %f8, [%r11];\n",
    "    mov.u32 %r11, sdata;\n",
    "    shl.b32 %r12, %r3, 2;\n",
    "    add.u32 %r11, %r11, %r12;\n",
    "    ld.shared.f32 %f9, [%r11];\n",
    "    add.rn.f32 %f9, %f9, %f8;\n",
    "    st.shared.f32 [%r11], %f9;\n",
    "SM_RED_SUM_SKIP:\n",
    "    bar.sync 0;\n",
    "    shr.u32 %r9, %r9, 1;\n",
    "    setp.ge.u32 %p0, %r9, 1;\n",
    "    @%p0 bra SM_RED_SUM;\n",
    "\n",
    "    // Broadcast sum\n",
    "    mov.u32 %r11, sdata;\n",
    "    ld.shared.f32 %f10, [%r11];  // row_sum\n",
    "    bar.sync 0;\n",
    "\n",
    "    // --- Phase 3: divide by sum ---\n",
    "    mov.u32 %r6, %r3;\n",
    "SM_DIV_LOOP:\n",
    "    setp.ge.u32 %p0, %r6, %r1;\n",
    "    @%p0 bra SM_DIV_DONE;\n",
    "    mul.wide.u32 %rd3, %r6, 4;\n",
    "    add.u64 %rd4, %rd2, %rd3;\n",
    "    ld.global.f32 %f11, [%rd4];\n",
    "    div.approx.f32 %f11, %f11, %f10;\n",
    "    st.global.f32 [%rd4], %f11;\n",
    "    add.u32 %r6, %r6, %r4;\n",
    "    bra SM_DIV_LOOP;\n",
    "SM_DIV_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // slice_columns: copy a column range from src to dst
    //
    // Parameters: src ptr (.u64), dst ptr (.u64), rows (.u32),
    //             src_cols (.u32), dst_cols (.u32), col_start (.u32)
    // Grid: (ceil(dst_cols/16), ceil(rows/16), 1), Block: (16, 16, 1)
    // -----------------------------------------------------------------------
    ".visible .entry slice_columns(\n",
    "    .param .u64 param_src,\n",
    "    .param .u64 param_dst,\n",
    "    .param .u32 param_rows,\n",
    "    .param .u32 param_src_cols,\n",
    "    .param .u32 param_dst_cols,\n",
    "    .param .u32 param_col_start\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<8>;\n",
    "    .reg .u32 %r<14>;\n",
    "    .reg .f32 %f0;\n",
    "    .reg .pred %p<3>;\n",
    "\n",
    "    ld.param.u64 %rd0, [param_src];\n",
    "    ld.param.u64 %rd1, [param_dst];\n",
    "    ld.param.u32 %r0, [param_rows];\n",
    "    ld.param.u32 %r1, [param_src_cols];\n",
    "    ld.param.u32 %r2, [param_dst_cols];\n",
    "    ld.param.u32 %r3, [param_col_start];\n",
    "\n",
    "    // c = blockIdx.x * blockDim.x + threadIdx.x\n",
    "    mov.u32 %r4, %ctaid.x;\n",
    "    shl.b32 %r4, %r4, 4;\n",
    "    mov.u32 %r5, %tid.x;\n",
    "    add.u32 %r4, %r4, %r5;       // dst column index\n",
    "    // r = blockIdx.y * blockDim.y + threadIdx.y\n",
    "    mov.u32 %r6, %ctaid.y;\n",
    "    shl.b32 %r6, %r6, 4;\n",
    "    mov.u32 %r7, %tid.y;\n",
    "    add.u32 %r6, %r6, %r7;       // row index\n",
    "\n",
    "    setp.ge.u32 %p0, %r4, %r2;   // c >= dst_cols?\n",
    "    setp.ge.u32 %p1, %r6, %r0;   // r >= rows?\n",
    "    or.pred %p2, %p0, %p1;\n",
    "    @%p2 bra SLICE_DONE;\n",
    "\n",
    "    // src offset: r * src_cols + (col_start + c)\n",
    "    add.u32 %r8, %r3, %r4;       // col_start + c\n",
    "    mul.lo.u32 %r9, %r6, %r1;\n",
    "    add.u32 %r9, %r9, %r8;\n",
    "    mul.wide.u32 %rd2, %r9, 4;\n",
    "    add.u64 %rd3, %rd0, %rd2;\n",
    "    ld.global.f32 %f0, [%rd3];\n",
    "\n",
    "    // dst offset: r * dst_cols + c\n",
    "    mul.lo.u32 %r10, %r6, %r2;\n",
    "    add.u32 %r10, %r10, %r4;\n",
    "    mul.wide.u32 %rd4, %r10, 4;\n",
    "    add.u64 %rd5, %rd1, %rd4;\n",
    "    st.global.f32 [%rd5], %f0;\n",
    "SLICE_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // scatter_columns: write src columns into dst at col_offset
    //
    // Parameters: dst ptr (.u64), src ptr (.u64), rows (.u32),
    //             dst_cols (.u32), src_cols (.u32), col_offset (.u32)
    // Grid: (ceil(src_cols/16), ceil(rows/16), 1), Block: (16, 16, 1)
    // -----------------------------------------------------------------------
    ".visible .entry scatter_columns(\n",
    "    .param .u64 param_dst,\n",
    "    .param .u64 param_src,\n",
    "    .param .u32 param_rows,\n",
    "    .param .u32 param_dst_cols,\n",
    "    .param .u32 param_src_cols,\n",
    "    .param .u32 param_col_offset\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<8>;\n",
    "    .reg .u32 %r<14>;\n",
    "    .reg .f32 %f0;\n",
    "    .reg .pred %p<3>;\n",
    "\n",
    "    ld.param.u64 %rd0, [param_dst];\n",
    "    ld.param.u64 %rd1, [param_src];\n",
    "    ld.param.u32 %r0, [param_rows];\n",
    "    ld.param.u32 %r1, [param_dst_cols];\n",
    "    ld.param.u32 %r2, [param_src_cols];\n",
    "    ld.param.u32 %r3, [param_col_offset];\n",
    "\n",
    "    mov.u32 %r4, %ctaid.x;\n",
    "    shl.b32 %r4, %r4, 4;\n",
    "    mov.u32 %r5, %tid.x;\n",
    "    add.u32 %r4, %r4, %r5;       // src column index\n",
    "    mov.u32 %r6, %ctaid.y;\n",
    "    shl.b32 %r6, %r6, 4;\n",
    "    mov.u32 %r7, %tid.y;\n",
    "    add.u32 %r6, %r6, %r7;       // row\n",
    "\n",
    "    setp.ge.u32 %p0, %r4, %r2;\n",
    "    setp.ge.u32 %p1, %r6, %r0;\n",
    "    or.pred %p2, %p0, %p1;\n",
    "    @%p2 bra SCATTER_DONE;\n",
    "\n",
    "    // src offset: r * src_cols + c\n",
    "    mul.lo.u32 %r8, %r6, %r2;\n",
    "    add.u32 %r8, %r8, %r4;\n",
    "    mul.wide.u32 %rd2, %r8, 4;\n",
    "    add.u64 %rd3, %rd1, %rd2;\n",
    "    ld.global.f32 %f0, [%rd3];\n",
    "\n",
    "    // dst offset: r * dst_cols + (col_offset + c)\n",
    "    add.u32 %r9, %r3, %r4;\n",
    "    mul.lo.u32 %r10, %r6, %r1;\n",
    "    add.u32 %r10, %r10, %r9;\n",
    "    mul.wide.u32 %rd4, %r10, 4;\n",
    "    add.u64 %rd5, %rd0, %rd4;\n",
    "    st.global.f32 [%rd5], %f0;\n",
    "SCATTER_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // attention_mask: set scores[i*cols+j] = -10000.0 where mask[j] == 0
    //
    // Parameters: scores ptr (.u64), mask ptr (.u64), rows (.u32), cols (.u32)
    // Grid: (rows, ceil(cols/256), 1), Block: (256, 1, 1)
    // -----------------------------------------------------------------------
    ".visible .entry attention_mask(\n",
    "    .param .u64 param_scores,\n",
    "    .param .u64 param_mask,\n",
    "    .param .u32 param_rows,\n",
    "    .param .u32 param_cols\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<6>;\n",
    "    .reg .u32 %r<10>;\n",
    "    .reg .f32 %f0;\n",
    "    .reg .pred %p<3>;\n",
    "\n",
    "    ld.param.u64 %rd0, [param_scores];\n",
    "    ld.param.u64 %rd1, [param_mask];\n",
    "    ld.param.u32 %r0, [param_rows];\n",
    "    ld.param.u32 %r1, [param_cols];\n",
    "\n",
    "    // r = blockIdx.x, c = blockIdx.y * blockDim.x + threadIdx.x\n",
    "    mov.u32 %r5, %ctaid.x;       // r\n",
    "    mov.u32 %r2, %ctaid.y;\n",
    "    mov.u32 %r3, %ntid.x;\n",
    "    mul.lo.u32 %r2, %r2, %r3;\n",
    "    mov.u32 %r4, %tid.x;\n",
    "    add.u32 %r2, %r2, %r4;       // c\n",
    "\n",
    "    setp.ge.u32 %p0, %r2, %r1;\n",
    "    @%p0 bra MASK_DONE;\n",
    "\n",
    "    // Load mask[c]\n",
    "    mul.wide.u32 %rd2, %r2, 4;\n",
    "    add.u64 %rd3, %rd1, %rd2;\n",
    "    ld.global.u32 %r6, [%rd3];\n",
    "    setp.ne.u32 %p1, %r6, 0;     // mask[c] != 0 => skip\n",
    "    @%p1 bra MASK_DONE;\n",
    "\n",
    "    // scores[r*cols + c] = -10000.0\n",
    "    mul.lo.u32 %r7, %r5, %r1;\n",
    "    add.u32 %r7, %r7, %r2;\n",
    "    mul.wide.u32 %rd4, %r7, 4;\n",
    "    add.u64 %rd5, %rd0, %rd4;\n",
    "    mov.f32 %f0, 0fC61C4000;     // -10000.0\n",
    "    st.global.f32 [%rd5], %f0;\n",
    "MASK_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // mean_pool: sum rows where mask==1, divide by count
    //
    // Parameters: hidden ptr (.u64), mask ptr (.u64), output ptr (.u64),
    //             rows (.u32), cols (.u32)
    //
    // Grid: (1, 1, 1), Block: (256, 1, 1)
    //
    // One block processes the entire operation:
    //   - Sequential over rows (check mask)
    //   - Parallel reduction over cols (accumulate sum)
    //   - Final divide by count
    // -----------------------------------------------------------------------
    ".visible .entry mean_pool(\n",
    "    .param .u64 param_hidden,\n",
    "    .param .u64 param_mask,\n",
    "    .param .u64 param_output,\n",
    "    .param .u32 param_rows,\n",
    "    .param .u32 param_cols\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<14>;\n",
    "    .reg .u32 %r<16>;\n",
    "    .reg .f32 %f<10>;\n",
    "    .reg .pred %p<4>;\n",
    "    .shared .align 4 .f32 sdata[256];\n",
    "\n",
    "    ld.param.u64 %rd0, [param_hidden];\n",
    "    ld.param.u64 %rd1, [param_mask];\n",
    "    ld.param.u64 %rd2, [param_output];\n",
    "    ld.param.u32 %r0, [param_rows];\n",
    "    ld.param.u32 %r1, [param_cols];\n",
    "\n",
    "    mov.u32 %r2, %tid.x;         // tid\n",
    "    mov.u32 %r3, %ntid.x;        // blockDim = 256\n",
    "\n",
    "    // Each thread maintains a partial sum for the columns it owns.\n",
    "    // We accumulate across rows where mask==1.\n",
    "    // After all rows, we write partial sums and count, then normalize.\n",
    "\n",
    "    mov.f32 %f0, 0f00000000;     // count (as float)\n",
    "\n",
    "    // Zero the output: each thread writes 0 for its columns\n",
    "    // We'll accumulate in registers, then write at end.\n",
    "    // But cols can be > 256, so we use output as accumulator.\n",
    "    // First zero the output array.\n",
    "    mov.u32 %r4, %r2;\n",
    "MP_ZERO_LOOP:\n",
    "    setp.ge.u32 %p0, %r4, %r1;\n",
    "    @%p0 bra MP_ZERO_DONE;\n",
    "    mul.wide.u32 %rd3, %r4, 4;\n",
    "    add.u64 %rd4, %rd2, %rd3;\n",
    "    mov.f32 %f1, 0f00000000;\n",
    "    st.global.f32 [%rd4], %f1;\n",
    "    add.u32 %r4, %r4, %r3;\n",
    "    bra MP_ZERO_LOOP;\n",
    "MP_ZERO_DONE:\n",
    "    bar.sync 0;\n",
    "\n",
    "    // Iterate over rows\n",
    "    mov.u32 %r5, 0;              // row = 0\n",
    "MP_ROW_LOOP:\n",
    "    setp.ge.u32 %p0, %r5, %r0;\n",
    "    @%p0 bra MP_ROW_DONE;\n",
    "\n",
    "    // Check mask[row] - only thread 0 reads, broadcasts via shared\n",
    "    // Actually, all threads can read it (it's the same value)\n",
    "    mul.wide.u32 %rd5, %r5, 4;\n",
    "    add.u64 %rd6, %rd1, %rd5;\n",
    "    ld.global.u32 %r6, [%rd6];\n",
    "    setp.eq.u32 %p1, %r6, 0;\n",
    "    @%p1 bra MP_ROW_SKIP;\n",
    "\n",
    "    // mask[row] == 1: add hidden[row*cols + c] to output[c]\n",
    "    // Only thread 0 increments count\n",
    "    setp.eq.u32 %p2, %r2, 0;\n",
    "    @!%p2 bra MP_SKIP_COUNT;\n",
    "    mov.f32 %f2, 0f3F800000;     // 1.0\n",
    "    add.rn.f32 %f0, %f0, %f2;\n",
    "MP_SKIP_COUNT:\n",
    "\n",
    "    // Row base\n",
    "    mul.lo.u32 %r7, %r5, %r1;\n",
    "    mul.wide.u32 %rd7, %r7, 4;\n",
    "    add.u64 %rd8, %rd0, %rd7;    // &hidden[row * cols]\n",
    "\n",
    "    mov.u32 %r8, %r2;            // c = tid\n",
    "MP_COL_LOOP:\n",
    "    setp.ge.u32 %p0, %r8, %r1;\n",
    "    @%p0 bra MP_COL_DONE;\n",
    "    mul.wide.u32 %rd9, %r8, 4;\n",
    "    add.u64 %rd10, %rd8, %rd9;\n",
    "    ld.global.f32 %f3, [%rd10];  // hidden[row*cols + c]\n",
    "    add.u64 %rd11, %rd2, %rd9;\n",
    "    ld.global.f32 %f4, [%rd11];  // output[c]\n",
    "    add.rn.f32 %f4, %f4, %f3;\n",
    "    st.global.f32 [%rd11], %f4;\n",
    "    add.u32 %r8, %r8, %r3;\n",
    "    bra MP_COL_LOOP;\n",
    "MP_COL_DONE:\n",
    "\n",
    "MP_ROW_SKIP:\n",
    "    bar.sync 0;\n",
    "    add.u32 %r5, %r5, 1;\n",
    "    bra MP_ROW_LOOP;\n",
    "MP_ROW_DONE:\n",
    "\n",
    "    // Broadcast count from thread 0 via shared memory\n",
    "    setp.eq.u32 %p0, %r2, 0;\n",
    "    @!%p0 bra MP_SKIP_STORE_CNT;\n",
    "    mov.u32 %r9, sdata;\n",
    "    st.shared.f32 [%r9], %f0;\n",
    "MP_SKIP_STORE_CNT:\n",
    "    bar.sync 0;\n",
    "    mov.u32 %r9, sdata;\n",
    "    ld.shared.f32 %f5, [%r9];    // count\n",
    "\n",
    "    // Guard: if count == 0, skip divide\n",
    "    mov.f32 %f6, 0f00000000;\n",
    "    setp.eq.f32 %p0, %f5, %f6;\n",
    "    @%p0 bra MP_FINAL_DONE;\n",
    "\n",
    "    // Divide output[c] by count\n",
    "    mov.u32 %r10, %r2;\n",
    "MP_DIV_LOOP:\n",
    "    setp.ge.u32 %p0, %r10, %r1;\n",
    "    @%p0 bra MP_FINAL_DONE;\n",
    "    mul.wide.u32 %rd12, %r10, 4;\n",
    "    add.u64 %rd13, %rd2, %rd12;\n",
    "    ld.global.f32 %f7, [%rd13];\n",
    "    div.approx.f32 %f7, %f7, %f5;\n",
    "    st.global.f32 [%rd13], %f7;\n",
    "    add.u32 %r10, %r10, %r3;\n",
    "    bra MP_DIV_LOOP;\n",
    "MP_FINAL_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // batched_gemm_transpose: block-diagonal C[b] = A[b] * B[b]^T
    //
    // A,B: (batch*S, K), C: (batch*S, S)
    // Uses %ctaid.z as batch index.
    // A offset: batch * S * K, B offset: batch * S * K, C offset: batch * S * S
    //
    // Parameters: A ptr (.u64), B ptr (.u64), C ptr (.u64),
    //             seq_len (.u32), K (.u32)
    // Grid:  (ceil(S/16), ceil(S/16), batch_size)
    // Block: (16, 16, 1)
    // -----------------------------------------------------------------------
    ".visible .entry batched_gemm_transpose(\n",
    "    .param .u64 param_A,\n",
    "    .param .u64 param_B,\n",
    "    .param .u64 param_C,\n",
    "    .param .u32 param_S,\n",
    "    .param .u32 param_K\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<24>;\n",
    "    .reg .u32 %r<32>;\n",
    "    .reg .f32 %f<10>;\n",
    "    .reg .pred %p<5>;\n",
    "    .shared .align 4 .f32 tile_A[256];\n", // 16*16
    "    .shared .align 4 .f32 tile_B[256];\n", // 16*16
    "\n",
    "    // Load parameters\n",
    "    ld.param.u64 %rd0, [param_A];\n",
    "    ld.param.u64 %rd1, [param_B];\n",
    "    ld.param.u64 %rd2, [param_C];\n",
    "    ld.param.u32 %r0, [param_S];\n", // seq_len
    "    ld.param.u32 %r1, [param_K];\n", // K
    "\n",
    "    // Thread indices\n",
    "    mov.u32 %r3, %tid.x;         // tx (column within tile)\n",
    "    mov.u32 %r4, %tid.y;         // ty (row within tile)\n",
    "    mov.u32 %r5, %ctaid.x;       // bx (block col)\n",
    "    mov.u32 %r6, %ctaid.y;       // by (block row)\n",
    "    mov.u32 %r25, %ctaid.z;      // batch index\n",
    "\n",
    "    // Global row/col within this batch's sub-matrix\n",
    "    shl.b32 %r7, %r6, 4;         // by * 16\n",
    "    add.u32 %r7, %r7, %r4;       // row = by*16 + ty\n",
    "    shl.b32 %r8, %r5, 4;         // bx * 16\n",
    "    add.u32 %r8, %r8, %r3;       // col = bx*16 + tx\n",
    "\n",
    "    // Compute batch offsets\n",
    "    // A offset: batch * S * K (bytes: *4 done at access)\n",
    "    mul.lo.u32 %r26, %r25, %r0;  // batch * S\n",
    "    mul.lo.u32 %r27, %r26, %r1;  // batch * S * K\n",
    "    mul.wide.u32 %rd10, %r27, 4;\n",
    "    add.u64 %rd0, %rd0, %rd10;   // A += batch * S * K * 4\n",
    "    // B offset: same as A (batch * S * K)\n",
    "    add.u64 %rd1, %rd1, %rd10;   // B += batch * S * K * 4\n",
    "    // C offset: batch * S * S\n",
    "    mul.lo.u32 %r28, %r26, %r0;  // batch * S * S\n",
    "    mul.wide.u32 %rd11, %r28, 4;\n",
    "    add.u64 %rd2, %rd2, %rd11;   // C += batch * S * S * 4\n",
    "\n",
    "    // Accumulator\n",
    "    mov.f32 %f0, 0f00000000;\n",
    "\n",
    "    // Shared memory index for this thread: ty*16+tx\n",
    "    shl.b32 %r9, %r4, 4;\n",
    "    add.u32 %r9, %r9, %r3;       // shmem_idx = ty*16+tx\n",
    "\n",
    "    // Tile loop: t = 0, 16, 32, ...\n",
    "    mov.u32 %r10, 0;             // t = 0\n",
    "BGT_TILE_LOOP:\n",
    "    // --- Load tile_A[ty][tx] = A[row][t+tx] ---\n",
    "    add.u32 %r11, %r10, %r3;     // t + tx\n",
    "    setp.lt.u32 %p0, %r7, %r0;   // row < S?\n",
    "    setp.lt.u32 %p1, %r11, %r1;  // t+tx < K?\n",
    "    and.pred %p2, %p0, %p1;\n",
    "    @!%p2 bra BGT_ZERO_A;\n",
    "    // A offset: row*K + (t+tx)\n",
    "    mul.lo.u32 %r12, %r7, %r1;\n",
    "    add.u32 %r12, %r12, %r11;\n",
    "    mul.wide.u32 %rd3, %r12, 4;\n",
    "    add.u64 %rd3, %rd0, %rd3;\n",
    "    ld.global.f32 %f1, [%rd3];\n",
    "    bra BGT_STORE_A;\n",
    "BGT_ZERO_A:\n",
    "    mov.f32 %f1, 0f00000000;\n",
    "BGT_STORE_A:\n",
    "    mov.u32 %r13, tile_A;\n",
    "    shl.b32 %r14, %r9, 2;        // shmem_idx * 4\n",
    "    add.u32 %r13, %r13, %r14;\n",
    "    st.shared.f32 [%r13], %f1;\n",
    "\n",
    "    // --- Load tile_B[ty][tx] = B[col][t+ty] (B transposed: stored as (S,K)) ---\n",
    "    add.u32 %r15, %r10, %r4;     // t + ty\n",
    "    setp.lt.u32 %p0, %r8, %r0;   // col < S?\n",
    "    setp.lt.u32 %p1, %r15, %r1;  // t+ty < K?\n",
    "    and.pred %p2, %p0, %p1;\n",
    "    @!%p2 bra BGT_ZERO_B;\n",
    "    // B offset: col*K + (t+ty)\n",
    "    mul.lo.u32 %r16, %r8, %r1;\n",
    "    add.u32 %r16, %r16, %r15;\n",
    "    mul.wide.u32 %rd4, %r16, 4;\n",
    "    add.u64 %rd4, %rd1, %rd4;\n",
    "    ld.global.f32 %f2, [%rd4];\n",
    "    bra BGT_STORE_B;\n",
    "BGT_ZERO_B:\n",
    "    mov.f32 %f2, 0f00000000;\n",
    "BGT_STORE_B:\n",
    "    mov.u32 %r17, tile_B;\n",
    "    shl.b32 %r18, %r9, 2;\n",
    "    add.u32 %r17, %r17, %r18;\n",
    "    st.shared.f32 [%r17], %f2;\n",
    "\n",
    "    bar.sync 0;\n",
    "\n",
    "    // --- Accumulate: acc += tile_A[ty][k] * tile_B[k][tx] ---\n",
    "    mov.u32 %r19, 0;             // k = 0\n",
    "BGT_K_LOOP:\n",
    "    shl.b32 %r20, %r4, 4;\n",
    "    add.u32 %r20, %r20, %r19;\n",
    "    shl.b32 %r20, %r20, 2;\n",
    "    mov.u32 %r21, tile_A;\n",
    "    add.u32 %r21, %r21, %r20;\n",
    "    ld.shared.f32 %f3, [%r21];\n",
    "    shl.b32 %r22, %r19, 4;\n",
    "    add.u32 %r22, %r22, %r3;\n",
    "    shl.b32 %r22, %r22, 2;\n",
    "    mov.u32 %r23, tile_B;\n",
    "    add.u32 %r23, %r23, %r22;\n",
    "    ld.shared.f32 %f4, [%r23];\n",
    "    fma.rn.f32 %f0, %f3, %f4, %f0;\n",
    "    add.u32 %r19, %r19, 1;\n",
    "    setp.lt.u32 %p0, %r19, 16;\n",
    "    @%p0 bra BGT_K_LOOP;\n",
    "\n",
    "    bar.sync 0;\n",
    "\n",
    "    add.u32 %r10, %r10, 16;\n",
    "    setp.lt.u32 %p0, %r10, %r1;  // t < K\n",
    "    @%p0 bra BGT_TILE_LOOP;\n",
    "\n",
    "    // --- Write result ---\n",
    "    setp.lt.u32 %p0, %r7, %r0;   // row < S\n",
    "    setp.lt.u32 %p1, %r8, %r0;   // col < S\n",
    "    and.pred %p2, %p0, %p1;\n",
    "    @!%p2 bra BGT_DONE;\n",
    "    mul.lo.u32 %r24, %r7, %r0;   // row * S\n",
    "    add.u32 %r24, %r24, %r8;\n",
    "    mul.wide.u32 %rd5, %r24, 4;\n",
    "    add.u64 %rd5, %rd2, %rd5;\n",
    "    st.global.f32 [%rd5], %f0;\n",
    "BGT_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // batched_gemm: block-diagonal C[b] = A[b] * B[b]
    //
    // A: (batch*S, S), B: (batch*S, K), C: (batch*S, K)
    // Uses %ctaid.z as batch index.
    // A offset: batch * S * S, B offset: batch * S * K, C offset: batch * S * K
    //
    // Parameters: A ptr (.u64), B ptr (.u64), C ptr (.u64),
    //             seq_len (.u32), K (.u32)
    // Grid:  (ceil(K/16), ceil(S/16), batch_size)
    // Block: (16, 16, 1)
    // -----------------------------------------------------------------------
    ".visible .entry batched_gemm(\n",
    "    .param .u64 param_A,\n",
    "    .param .u64 param_B,\n",
    "    .param .u64 param_C,\n",
    "    .param .u32 param_S,\n",
    "    .param .u32 param_K\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<24>;\n",
    "    .reg .u32 %r<32>;\n",
    "    .reg .f32 %f<10>;\n",
    "    .reg .pred %p<5>;\n",
    "    .shared .align 4 .f32 tile_A[256];\n", // 16*16
    "    .shared .align 4 .f32 tile_B[256];\n", // 16*16
    "\n",
    "    // Load parameters\n",
    "    ld.param.u64 %rd0, [param_A];\n",
    "    ld.param.u64 %rd1, [param_B];\n",
    "    ld.param.u64 %rd2, [param_C];\n",
    "    ld.param.u32 %r0, [param_S];\n", // seq_len
    "    ld.param.u32 %r1, [param_K];\n", // K
    "\n",
    "    // Thread indices\n",
    "    mov.u32 %r3, %tid.x;         // tx (column within tile)\n",
    "    mov.u32 %r4, %tid.y;         // ty (row within tile)\n",
    "    mov.u32 %r5, %ctaid.x;       // bx (block col)\n",
    "    mov.u32 %r6, %ctaid.y;       // by (block row)\n",
    "    mov.u32 %r25, %ctaid.z;      // batch index\n",
    "\n",
    "    // Global row/col within this batch's sub-matrix\n",
    "    shl.b32 %r7, %r6, 4;         // by * 16\n",
    "    add.u32 %r7, %r7, %r4;       // row = by*16 + ty\n",
    "    shl.b32 %r8, %r5, 4;         // bx * 16\n",
    "    add.u32 %r8, %r8, %r3;       // col = bx*16 + tx\n",
    "\n",
    "    // Compute batch offsets\n",
    "    // A offset: batch * S * S\n",
    "    mul.lo.u32 %r26, %r25, %r0;  // batch * S\n",
    "    mul.lo.u32 %r27, %r26, %r0;  // batch * S * S\n",
    "    mul.wide.u32 %rd10, %r27, 4;\n",
    "    add.u64 %rd0, %rd0, %rd10;   // A += batch * S * S * 4\n",
    "    // B offset: batch * S * K\n",
    "    mul.lo.u32 %r28, %r26, %r1;  // batch * S * K\n",
    "    mul.wide.u32 %rd11, %r28, 4;\n",
    "    add.u64 %rd1, %rd1, %rd11;   // B += batch * S * K * 4\n",
    "    // C offset: same as B (batch * S * K)\n",
    "    add.u64 %rd2, %rd2, %rd11;   // C += batch * S * K * 4\n",
    "\n",
    "    // Accumulator\n",
    "    mov.f32 %f0, 0f00000000;\n",
    "\n",
    "    // Shared memory index for this thread: ty*16+tx\n",
    "    shl.b32 %r9, %r4, 4;\n",
    "    add.u32 %r9, %r9, %r3;       // shmem_idx = ty*16+tx\n",
    "\n",
    "    // Tile loop over inner dimension S: t = 0, 16, 32, ...\n",
    "    mov.u32 %r10, 0;             // t = 0\n",
    "BG_TILE_LOOP:\n",
    "    // --- Load tile_A[ty][tx] = A[row][t+tx] ---\n",
    "    add.u32 %r11, %r10, %r3;     // t + tx\n",
    "    setp.lt.u32 %p0, %r7, %r0;   // row < S?\n",
    "    setp.lt.u32 %p1, %r11, %r0;  // t+tx < S? (inner dim is S)\n",
    "    and.pred %p2, %p0, %p1;\n",
    "    @!%p2 bra BG_ZERO_A;\n",
    "    // A offset: row*S + (t+tx)\n",
    "    mul.lo.u32 %r12, %r7, %r0;\n",
    "    add.u32 %r12, %r12, %r11;\n",
    "    mul.wide.u32 %rd3, %r12, 4;\n",
    "    add.u64 %rd3, %rd0, %rd3;\n",
    "    ld.global.f32 %f1, [%rd3];\n",
    "    bra BG_STORE_A;\n",
    "BG_ZERO_A:\n",
    "    mov.f32 %f1, 0f00000000;\n",
    "BG_STORE_A:\n",
    "    mov.u32 %r13, tile_A;\n",
    "    shl.b32 %r14, %r9, 2;        // shmem_idx * 4\n",
    "    add.u32 %r13, %r13, %r14;\n",
    "    st.shared.f32 [%r13], %f1;\n",
    "\n",
    "    // --- Load tile_B[ty][tx] = B[t+ty][col] ---\n",
    "    add.u32 %r15, %r10, %r4;     // t + ty\n",
    "    setp.lt.u32 %p0, %r15, %r0;  // t+ty < S? (inner dim is S)\n",
    "    setp.lt.u32 %p1, %r8, %r1;   // col < K?\n",
    "    and.pred %p2, %p0, %p1;\n",
    "    @!%p2 bra BG_ZERO_B;\n",
    "    // B offset: (t+ty)*K + col\n",
    "    mul.lo.u32 %r16, %r15, %r1;\n",
    "    add.u32 %r16, %r16, %r8;\n",
    "    mul.wide.u32 %rd4, %r16, 4;\n",
    "    add.u64 %rd4, %rd1, %rd4;\n",
    "    ld.global.f32 %f2, [%rd4];\n",
    "    bra BG_STORE_B;\n",
    "BG_ZERO_B:\n",
    "    mov.f32 %f2, 0f00000000;\n",
    "BG_STORE_B:\n",
    "    mov.u32 %r17, tile_B;\n",
    "    shl.b32 %r18, %r9, 2;\n",
    "    add.u32 %r17, %r17, %r18;\n",
    "    st.shared.f32 [%r17], %f2;\n",
    "\n",
    "    bar.sync 0;\n",
    "\n",
    "    // --- Accumulate: acc += tile_A[ty][k] * tile_B[k][tx] ---\n",
    "    mov.u32 %r19, 0;             // k = 0\n",
    "BG_K_LOOP:\n",
    "    shl.b32 %r20, %r4, 4;\n",
    "    add.u32 %r20, %r20, %r19;\n",
    "    shl.b32 %r20, %r20, 2;\n",
    "    mov.u32 %r21, tile_A;\n",
    "    add.u32 %r21, %r21, %r20;\n",
    "    ld.shared.f32 %f3, [%r21];\n",
    "    shl.b32 %r22, %r19, 4;\n",
    "    add.u32 %r22, %r22, %r3;\n",
    "    shl.b32 %r22, %r22, 2;\n",
    "    mov.u32 %r23, tile_B;\n",
    "    add.u32 %r23, %r23, %r22;\n",
    "    ld.shared.f32 %f4, [%r23];\n",
    "    fma.rn.f32 %f0, %f3, %f4, %f0;\n",
    "    add.u32 %r19, %r19, 1;\n",
    "    setp.lt.u32 %p0, %r19, 16;\n",
    "    @%p0 bra BG_K_LOOP;\n",
    "\n",
    "    bar.sync 0;\n",
    "\n",
    "    add.u32 %r10, %r10, 16;\n",
    "    setp.lt.u32 %p0, %r10, %r0;  // t < S\n",
    "    @%p0 bra BG_TILE_LOOP;\n",
    "\n",
    "    // --- Write result ---\n",
    "    setp.lt.u32 %p0, %r7, %r0;   // row < S\n",
    "    setp.lt.u32 %p1, %r8, %r1;   // col < K\n",
    "    and.pred %p2, %p0, %p1;\n",
    "    @!%p2 bra BG_DONE;\n",
    "    mul.lo.u32 %r24, %r7, %r1;   // row * K\n",
    "    add.u32 %r24, %r24, %r8;\n",
    "    mul.wide.u32 %rd5, %r24, 4;\n",
    "    add.u64 %rd5, %rd2, %rd5;\n",
    "    st.global.f32 [%rd5], %f0;\n",
    "BG_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // batched_attention_mask: per-sequence attention masking
    //
    // For each row r: batch = r / seq_len,
    //   if mask[batch * seq_len + col] == 0, set scores[r*cols+col] = -10000
    //
    // Parameters: scores ptr (.u64), mask ptr (.u64),
    //             total_rows (.u32), seq_len (.u32)
    // Grid:  (total_rows, ceil(seq_len/256), 1)
    // Block: (256, 1, 1)
    // -----------------------------------------------------------------------
    ".visible .entry batched_attention_mask(\n",
    "    .param .u64 param_scores,\n",
    "    .param .u64 param_mask,\n",
    "    .param .u32 param_total_rows,\n",
    "    .param .u32 param_seq_len\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<8>;\n",
    "    .reg .u32 %r<14>;\n",
    "    .reg .f32 %f0;\n",
    "    .reg .pred %p<3>;\n",
    "\n",
    "    ld.param.u64 %rd0, [param_scores];\n",
    "    ld.param.u64 %rd1, [param_mask];\n",
    "    ld.param.u32 %r0, [param_total_rows];\n",
    "    ld.param.u32 %r1, [param_seq_len];\n",
    "\n",
    "    // r = blockIdx.x, col = blockIdx.y * blockDim.x + threadIdx.x\n",
    "    mov.u32 %r5, %ctaid.x;       // row\n",
    "    mov.u32 %r2, %ctaid.y;\n",
    "    mov.u32 %r3, %ntid.x;\n",
    "    mul.lo.u32 %r2, %r2, %r3;\n",
    "    mov.u32 %r4, %tid.x;\n",
    "    add.u32 %r2, %r2, %r4;       // col\n",
    "\n",
    "    // Bounds check: col < seq_len\n",
    "    setp.ge.u32 %p0, %r2, %r1;\n",
    "    @%p0 bra BAM_DONE;\n",
    "\n",
    "    // batch = r / seq_len\n",
    "    div.u32 %r6, %r5, %r1;       // batch\n",
    "    // mask index = batch * seq_len + col\n",
    "    mul.lo.u32 %r7, %r6, %r1;\n",
    "    add.u32 %r7, %r7, %r2;       // mask_idx\n",
    "\n",
    "    // Load mask[mask_idx]\n",
    "    mul.wide.u32 %rd2, %r7, 4;\n",
    "    add.u64 %rd3, %rd1, %rd2;\n",
    "    ld.global.u32 %r8, [%rd3];\n",
    "    setp.ne.u32 %p1, %r8, 0;     // mask != 0 => skip\n",
    "    @%p1 bra BAM_DONE;\n",
    "\n",
    "    // scores[r * seq_len + col] = -10000.0\n",
    "    mul.lo.u32 %r9, %r5, %r1;\n",
    "    add.u32 %r9, %r9, %r2;\n",
    "    mul.wide.u32 %rd4, %r9, 4;\n",
    "    add.u64 %rd5, %rd0, %rd4;\n",
    "    mov.f32 %f0, 0fC61C4000;     // -10000.0\n",
    "    st.global.f32 [%rd5], %f0;\n",
    "BAM_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // batched_mean_pool: per-sequence mean pooling
    //
    // hidden: (batch*S, D), mask: (batch*S,)
    // One block per batch - each block processes rows [batch*S..(batch+1)*S]
    // Each thread handles columns at stride blockDim.x, accumulates sum
    // for masked rows, divides by count.
    // Output: (batch, D) - output offset = batch * cols
    //
    // Parameters: hidden ptr (.u64), mask ptr (.u64), output ptr (.u64),
    //             seq_len (.u32), cols (.u32)
    // Grid:  (1, batch_size, 1)
    // Block: (256, 1, 1)
    // -----------------------------------------------------------------------
    ".visible .entry batched_mean_pool(\n",
    "    .param .u64 param_hidden,\n",
    "    .param .u64 param_mask,\n",
    "    .param .u64 param_output,\n",
    "    .param .u32 param_seq_len,\n",
    "    .param .u32 param_cols\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<16>;\n",
    "    .reg .u32 %r<20>;\n",
    "    .reg .f32 %f<10>;\n",
    "    .reg .pred %p<4>;\n",
    "    .shared .align 4 .f32 sdata[256];\n",
    "\n",
    "    ld.param.u64 %rd0, [param_hidden];\n",
    "    ld.param.u64 %rd1, [param_mask];\n",
    "    ld.param.u64 %rd2, [param_output];\n",
    "    ld.param.u32 %r0, [param_seq_len];\n",
    "    ld.param.u32 %r1, [param_cols];\n",
    "\n",
    "    mov.u32 %r2, %tid.x;         // tid\n",
    "    mov.u32 %r3, %ntid.x;        // blockDim = 256\n",
    "    mov.u32 %r14, %ctaid.y;      // batch index\n",
    "\n",
    "    // Compute base offsets for this batch\n",
    "    // row_base = batch * seq_len\n",
    "    mul.lo.u32 %r15, %r14, %r0;  // batch * S\n",
    "    // hidden base: row_base * cols\n",
    "    mul.lo.u32 %r16, %r15, %r1;  // batch * S * cols\n",
    "    mul.wide.u32 %rd3, %r16, 4;\n",
    "    add.u64 %rd4, %rd0, %rd3;    // &hidden[batch*S*cols]\n",
    "    // mask base: row_base\n",
    "    mul.wide.u32 %rd5, %r15, 4;\n",
    "    add.u64 %rd6, %rd1, %rd5;    // &mask[batch*S]\n",
    "    // output base: batch * cols\n",
    "    mul.lo.u32 %r17, %r14, %r1;  // batch * cols\n",
    "    mul.wide.u32 %rd7, %r17, 4;\n",
    "    add.u64 %rd8, %rd2, %rd7;    // &output[batch*cols]\n",
    "\n",
    "    // Zero the output for this batch\n",
    "    mov.f32 %f0, 0f00000000;     // count (as float)\n",
    "    mov.u32 %r4, %r2;\n",
    "BMP_ZERO_LOOP:\n",
    "    setp.ge.u32 %p0, %r4, %r1;\n",
    "    @%p0 bra BMP_ZERO_DONE;\n",
    "    mul.wide.u32 %rd9, %r4, 4;\n",
    "    add.u64 %rd10, %rd8, %rd9;\n",
    "    mov.f32 %f1, 0f00000000;\n",
    "    st.global.f32 [%rd10], %f1;\n",
    "    add.u32 %r4, %r4, %r3;\n",
    "    bra BMP_ZERO_LOOP;\n",
    "BMP_ZERO_DONE:\n",
    "    bar.sync 0;\n",
    "\n",
    "    // Iterate over rows within this batch\n",
    "    mov.u32 %r5, 0;              // local_row = 0\n",
    "BMP_ROW_LOOP:\n",
    "    setp.ge.u32 %p0, %r5, %r0;\n",
    "    @%p0 bra BMP_ROW_DONE;\n",
    "\n",
    "    // Check mask[local_row]\n",
    "    mul.wide.u32 %rd11, %r5, 4;\n",
    "    add.u64 %rd12, %rd6, %rd11;\n",
    "    ld.global.u32 %r6, [%rd12];\n",
    "    setp.eq.u32 %p1, %r6, 0;\n",
    "    @%p1 bra BMP_ROW_SKIP;\n",
    "\n",
    "    // mask == 1: increment count (thread 0 only)\n",
    "    setp.eq.u32 %p2, %r2, 0;\n",
    "    @!%p2 bra BMP_SKIP_COUNT;\n",
    "    mov.f32 %f2, 0f3F800000;     // 1.0\n",
    "    add.rn.f32 %f0, %f0, %f2;\n",
    "BMP_SKIP_COUNT:\n",
    "\n",
    "    // Row base for hidden: local_row * cols\n",
    "    mul.lo.u32 %r7, %r5, %r1;\n",
    "    mul.wide.u32 %rd13, %r7, 4;\n",
    "    add.u64 %rd14, %rd4, %rd13;  // &hidden[batch*S*cols + local_row*cols]\n",
    "\n",
    "    mov.u32 %r8, %r2;            // c = tid\n",
    "BMP_COL_LOOP:\n",
    "    setp.ge.u32 %p0, %r8, %r1;\n",
    "    @%p0 bra BMP_COL_DONE;\n",
    "    mul.wide.u32 %rd9, %r8, 4;\n",
    "    add.u64 %rd10, %rd14, %rd9;\n",
    "    ld.global.f32 %f3, [%rd10];  // hidden value\n",
    "    add.u64 %rd15, %rd8, %rd9;\n",
    "    ld.global.f32 %f4, [%rd15];  // output accumulator\n",
    "    add.rn.f32 %f4, %f4, %f3;\n",
    "    st.global.f32 [%rd15], %f4;\n",
    "    add.u32 %r8, %r8, %r3;\n",
    "    bra BMP_COL_LOOP;\n",
    "BMP_COL_DONE:\n",
    "\n",
    "BMP_ROW_SKIP:\n",
    "    bar.sync 0;\n",
    "    add.u32 %r5, %r5, 1;\n",
    "    bra BMP_ROW_LOOP;\n",
    "BMP_ROW_DONE:\n",
    "\n",
    "    // Broadcast count from thread 0 via shared memory\n",
    "    setp.eq.u32 %p0, %r2, 0;\n",
    "    @!%p0 bra BMP_SKIP_STORE_CNT;\n",
    "    mov.u32 %r9, sdata;\n",
    "    st.shared.f32 [%r9], %f0;\n",
    "BMP_SKIP_STORE_CNT:\n",
    "    bar.sync 0;\n",
    "    mov.u32 %r9, sdata;\n",
    "    ld.shared.f32 %f5, [%r9];    // count\n",
    "\n",
    "    // Guard: if count == 0, skip divide\n",
    "    mov.f32 %f6, 0f00000000;\n",
    "    setp.eq.f32 %p0, %f5, %f6;\n",
    "    @%p0 bra BMP_FINAL_DONE;\n",
    "\n",
    "    // Divide output[c] by count\n",
    "    mov.u32 %r10, %r2;\n",
    "BMP_DIV_LOOP:\n",
    "    setp.ge.u32 %p0, %r10, %r1;\n",
    "    @%p0 bra BMP_FINAL_DONE;\n",
    "    mul.wide.u32 %rd9, %r10, 4;\n",
    "    add.u64 %rd10, %rd8, %rd9;\n",
    "    ld.global.f32 %f7, [%rd10];\n",
    "    div.approx.f32 %f7, %f7, %f5;\n",
    "    st.global.f32 [%rd10], %f7;\n",
    "    add.u32 %r10, %r10, %r3;\n",
    "    bra BMP_DIV_LOOP;\n",
    "BMP_FINAL_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // transpose_heads: (B*S, H*D) -> (B*H*S, D)
    //
    // Rearranges Q/K/V so all heads are in the batch dimension.
    // Input row `r`, column `head*D + d` maps to:
    //   output row: batch*H*S + head*S + token, column: d
    // where batch = r / S, token = r % S.
    //
    // Parameters: src ptr (.u64), dst ptr (.u64),
    //             batch_size (.u32), seq_len (.u32),
    //             num_heads (.u32), head_dim (.u32)
    // Grid:  (ceil(D/16), ceil(B*S/16), H)
    // Block: (16, 16, 1)
    // -----------------------------------------------------------------------
    ".visible .entry transpose_heads(\n",
    "    .param .u64 param_src,\n",
    "    .param .u64 param_dst,\n",
    "    .param .u32 param_batch_size,\n",
    "    .param .u32 param_seq_len,\n",
    "    .param .u32 param_num_heads,\n",
    "    .param .u32 param_head_dim\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<10>;\n",
    "    .reg .u32 %r<24>;\n",
    "    .reg .f32 %f0;\n",
    "    .reg .pred %p<4>;\n",
    "\n",
    "    ld.param.u64 %rd0, [param_src];\n",
    "    ld.param.u64 %rd1, [param_dst];\n",
    "    ld.param.u32 %r0, [param_batch_size];\n",
    "    ld.param.u32 %r1, [param_seq_len];\n",
    "    ld.param.u32 %r2, [param_num_heads];\n",
    "    ld.param.u32 %r3, [param_head_dim];\n",
    "\n",
    "    // d = blockIdx.x * 16 + threadIdx.x\n",
    "    mov.u32 %r4, %ctaid.x;\n",
    "    shl.b32 %r4, %r4, 4;\n",
    "    mov.u32 %r5, %tid.x;\n",
    "    add.u32 %r4, %r4, %r5;       // d (column in head)\n",
    "    // r = blockIdx.y * 16 + threadIdx.y (row in B*S)\n",
    "    mov.u32 %r6, %ctaid.y;\n",
    "    shl.b32 %r6, %r6, 4;\n",
    "    mov.u32 %r7, %tid.y;\n",
    "    add.u32 %r6, %r6, %r7;       // r (input row)\n",
    "    // head = blockIdx.z\n",
    "    mov.u32 %r8, %ctaid.z;       // head\n",
    "\n",
    "    // total_rows = batch_size * seq_len\n",
    "    mul.lo.u32 %r9, %r0, %r1;    // B*S\n",
    "\n",
    "    // Bounds check: d < head_dim && r < B*S\n",
    "    setp.ge.u32 %p0, %r4, %r3;   // d >= D?\n",
    "    setp.ge.u32 %p1, %r6, %r9;   // r >= B*S?\n",
    "    or.pred %p2, %p0, %p1;\n",
    "    @%p2 bra TH_DONE;\n",
    "\n",
    "    // src_cols = num_heads * head_dim\n",
    "    mul.lo.u32 %r10, %r2, %r3;   // H*D\n",
    "    // src offset: r * (H*D) + head*D + d\n",
    "    mul.lo.u32 %r11, %r6, %r10;  // r * H*D\n",
    "    mul.lo.u32 %r12, %r8, %r3;   // head * D\n",
    "    add.u32 %r11, %r11, %r12;\n",
    "    add.u32 %r11, %r11, %r4;     // + d\n",
    "    mul.wide.u32 %rd2, %r11, 4;\n",
    "    add.u64 %rd3, %rd0, %rd2;\n",
    "    ld.global.f32 %f0, [%rd3];\n",
    "\n",
    "    // batch = r / S, token = r % S\n",
    "    div.u32 %r13, %r6, %r1;      // batch\n",
    "    rem.u32 %r14, %r6, %r1;      // token\n",
    "\n",
    "    // dst row: batch*H*S + head*S + token\n",
    "    mul.lo.u32 %r15, %r13, %r2;  // batch*H\n",
    "    add.u32 %r15, %r15, %r8;     // + head\n",
    "    mul.lo.u32 %r15, %r15, %r1;  // * S\n",
    "    add.u32 %r15, %r15, %r14;    // + token\n",
    "    // dst offset: dst_row * D + d\n",
    "    mul.lo.u32 %r16, %r15, %r3;  // dst_row * D\n",
    "    add.u32 %r16, %r16, %r4;     // + d\n",
    "    mul.wide.u32 %rd4, %r16, 4;\n",
    "    add.u64 %rd5, %rd1, %rd4;\n",
    "    st.global.f32 [%rd5], %f0;\n",
    "TH_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // untranspose_heads: (B*H*S, D) -> (B*S, H*D)
    //
    // Inverse of transpose_heads. For each output position
    // (r, head*D + d) where r is in [0, B*S), we read from
    // input row batch*H*S + head*S + token, column d.
    //
    // Parameters: same as transpose_heads
    // Grid:  (ceil(D/16), ceil(B*S/16), H)
    // Block: (16, 16, 1)
    // -----------------------------------------------------------------------
    ".visible .entry untranspose_heads(\n",
    "    .param .u64 param_src,\n",
    "    .param .u64 param_dst,\n",
    "    .param .u32 param_batch_size,\n",
    "    .param .u32 param_seq_len,\n",
    "    .param .u32 param_num_heads,\n",
    "    .param .u32 param_head_dim\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<10>;\n",
    "    .reg .u32 %r<24>;\n",
    "    .reg .f32 %f0;\n",
    "    .reg .pred %p<4>;\n",
    "\n",
    "    ld.param.u64 %rd0, [param_src];\n", // src: (B*H*S, D)
    "    ld.param.u64 %rd1, [param_dst];\n", // dst: (B*S, H*D)
    "    ld.param.u32 %r0, [param_batch_size];\n",
    "    ld.param.u32 %r1, [param_seq_len];\n",
    "    ld.param.u32 %r2, [param_num_heads];\n",
    "    ld.param.u32 %r3, [param_head_dim];\n",
    "\n",
    "    // d = blockIdx.x * 16 + threadIdx.x\n",
    "    mov.u32 %r4, %ctaid.x;\n",
    "    shl.b32 %r4, %r4, 4;\n",
    "    mov.u32 %r5, %tid.x;\n",
    "    add.u32 %r4, %r4, %r5;       // d\n",
    "    // r = blockIdx.y * 16 + threadIdx.y (output row in B*S)\n",
    "    mov.u32 %r6, %ctaid.y;\n",
    "    shl.b32 %r6, %r6, 4;\n",
    "    mov.u32 %r7, %tid.y;\n",
    "    add.u32 %r6, %r6, %r7;       // r\n",
    "    // head = blockIdx.z\n",
    "    mov.u32 %r8, %ctaid.z;       // head\n",
    "\n",
    "    // total_rows = batch_size * seq_len\n",
    "    mul.lo.u32 %r9, %r0, %r1;\n",
    "\n",
    "    // Bounds check\n",
    "    setp.ge.u32 %p0, %r4, %r3;\n",
    "    setp.ge.u32 %p1, %r6, %r9;\n",
    "    or.pred %p2, %p0, %p1;\n",
    "    @%p2 bra UTH_DONE;\n",
    "\n",
    "    // batch = r / S, token = r % S\n",
    "    div.u32 %r13, %r6, %r1;\n",
    "    rem.u32 %r14, %r6, %r1;\n",
    "\n",
    "    // src row: batch*H*S + head*S + token\n",
    "    mul.lo.u32 %r15, %r13, %r2;  // batch*H\n",
    "    add.u32 %r15, %r15, %r8;     // + head\n",
    "    mul.lo.u32 %r15, %r15, %r1;  // * S\n",
    "    add.u32 %r15, %r15, %r14;    // + token\n",
    "    // src offset: src_row * D + d\n",
    "    mul.lo.u32 %r16, %r15, %r3;\n",
    "    add.u32 %r16, %r16, %r4;\n",
    "    mul.wide.u32 %rd2, %r16, 4;\n",
    "    add.u64 %rd3, %rd0, %rd2;\n",
    "    ld.global.f32 %f0, [%rd3];\n",
    "\n",
    "    // dst_cols = H*D\n",
    "    mul.lo.u32 %r10, %r2, %r3;\n",
    "    // dst offset: r * (H*D) + head*D + d\n",
    "    mul.lo.u32 %r11, %r6, %r10;\n",
    "    mul.lo.u32 %r12, %r8, %r3;\n",
    "    add.u32 %r11, %r11, %r12;\n",
    "    add.u32 %r11, %r11, %r4;\n",
    "    mul.wide.u32 %rd4, %r11, 4;\n",
    "    add.u64 %rd5, %rd1, %rd4;\n",
    "    st.global.f32 [%rd5], %f0;\n",
    "UTH_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    // -----------------------------------------------------------------------
    // multi_head_batched_attention_mask: per-sequence masking for multi-head
    //
    // Like batched_attention_mask but for (B*H*S, S) score matrices where
    // every H consecutive batches share one mask row.
    //
    // For row r: group = r / (H * S), i.e. the original batch index.
    //   mask_idx = group * S + col
    //   if mask[mask_idx] == 0, scores[r*cols+col] = -10000
    //
    // Parameters: scores ptr (.u64), mask ptr (.u64),
    //             total_rows (.u32), seq_len (.u32), num_heads (.u32)
    // Grid:  (total_rows, ceil(seq_len/256), 1)
    // Block: (256, 1, 1)
    // -----------------------------------------------------------------------
    ".visible .entry multi_head_batched_attention_mask(\n",
    "    .param .u64 param_scores,\n",
    "    .param .u64 param_mask,\n",
    "    .param .u32 param_total_rows,\n",
    "    .param .u32 param_seq_len,\n",
    "    .param .u32 param_num_heads\n",
    ")\n",
    "{\n",
    "    .reg .u64 %rd<8>;\n",
    "    .reg .u32 %r<16>;\n",
    "    .reg .f32 %f0;\n",
    "    .reg .pred %p<3>;\n",
    "\n",
    "    ld.param.u64 %rd0, [param_scores];\n",
    "    ld.param.u64 %rd1, [param_mask];\n",
    "    ld.param.u32 %r0, [param_total_rows];\n",
    "    ld.param.u32 %r1, [param_seq_len];\n",
    "    ld.param.u32 %r2, [param_num_heads];\n",
    "\n",
    "    // r = blockIdx.x, col = blockIdx.y * blockDim.x + threadIdx.x\n",
    "    mov.u32 %r6, %ctaid.x;       // row\n",
    "    mov.u32 %r3, %ctaid.y;\n",
    "    mov.u32 %r4, %ntid.x;\n",
    "    mul.lo.u32 %r3, %r3, %r4;\n",
    "    mov.u32 %r5, %tid.x;\n",
    "    add.u32 %r3, %r3, %r5;       // col\n",
    "\n",
    "    // Bounds check: col < seq_len\n",
    "    setp.ge.u32 %p0, %r3, %r1;\n",
    "    @%p0 bra MHBAM_DONE;\n",
    "\n",
    "    // group_size = H * S\n",
    "    mul.lo.u32 %r7, %r2, %r1;    // H * S\n",
    "    // group = r / (H * S)  (original batch index)\n",
    "    div.u32 %r8, %r6, %r7;       // group\n",
    "    // mask_idx = group * S + col\n",
    "    mul.lo.u32 %r9, %r8, %r1;\n",
    "    add.u32 %r9, %r9, %r3;       // mask_idx\n",
    "\n",
    "    // Load mask[mask_idx]\n",
    "    mul.wide.u32 %rd2, %r9, 4;\n",
    "    add.u64 %rd3, %rd1, %rd2;\n",
    "    ld.global.u32 %r10, [%rd3];\n",
    "    setp.ne.u32 %p1, %r10, 0;    // mask != 0 => skip\n",
    "    @%p1 bra MHBAM_DONE;\n",
    "\n",
    "    // scores[r * seq_len + col] = -10000.0\n",
    "    mul.lo.u32 %r11, %r6, %r1;\n",
    "    add.u32 %r11, %r11, %r3;\n",
    "    mul.wide.u32 %rd4, %r11, 4;\n",
    "    add.u64 %rd5, %rd0, %rd4;\n",
    "    mov.f32 %f0, 0fC61C4000;     // -10000.0\n",
    "    st.global.f32 [%rd5], %f0;\n",
    "MHBAM_DONE:\n",
    "    ret;\n",
    "}\n",
    "\n",
    "\0" // Null terminator for cuModuleLoadData
);
